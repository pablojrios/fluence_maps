{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pablojrios/fluence_maps/blob/master/tf2_transfer_learning_gamma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTa3Ee15WsJ"
   },
   "source": [
    "# Transfer learning with a pretrained ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wRK8ctZQIEuc"
   },
   "outputs": [],
   "source": [
    "def isGoogleColab():\n",
    "    # 'ipykernel.zmqshell' runs in our server\n",
    "    # 'google.colab._shell' runs in Google Colab\n",
    "    return get_ipython().__class__.__module__ == 'google.colab._shell'\n",
    "\n",
    "#import sys\n",
    "#import IPython\n",
    "\n",
    "#if 'ipykernel' in sys.modules:\n",
    "#    ip = sys.modules['ipykernel']\n",
    "#    ip_version = ip.version_info\n",
    "#    ip_client = ip.write_connection_file.__module__.split('.')[0]\n",
    "\n",
    "#ip_version, ip_client\n",
    "\n",
    "#ip_version = IPython.utils.sysinfo.get_sys_info()['ipython_version']\n",
    "#ip_version\n",
    "\n",
    "#if 'IPython' in sys.modules:\n",
    "#    ip = sys.modules['IPython']\n",
    "#    ip_version = ip.version_info\n",
    "#    print(ip_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iBMcobPHdD8O"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "from random import shuffle, randrange\n",
    "import random\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version = 2.2.0, addons version = 0.10.0\n",
      "Executing eagerly = True\n",
      "OpenCV version = 3.4.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'04/02/2021 16:21:23'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version = {}, addons version = {}'.format(tf.__version__, tfa.__version__))\n",
    "print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "keras = tf.keras\n",
    "\n",
    "import cv2 # to perform data augmentation\n",
    "print('OpenCV version = {}'.format(cv2.__version__))\n",
    "\n",
    "datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q50x39yF5BPt"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "if isGoogleColab():\n",
    "    %cd '/content'\n",
    "    if os.path.exists('fluence_maps'):\n",
    "      !rm -fr fluence_maps\n",
    "\n",
    "    ## Install required dependencies\n",
    "    !pip install -q pydicom\n",
    "    ## to support ResNet18 and ResNet34\n",
    "    !pip install image-classifiers\n",
    "    ## https://github.com/tensorflow/addons/issues/2251\n",
    "    !pip install -U tensorflow-addons\n",
    "\n",
    "    GIT_USERNAME = \"pablojrios\"\n",
    "    GIT_TOKEN = \"1d88a0b85d2b00a03796e4d8b7e5f7b249b12f9b\"\n",
    "    !git clone -s https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/fluence_maps.git\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    %cd -q '/content/fluence_maps'\n",
    "    \n",
    "    ARG_DATASET_DIR='/content/drive/My Drive/Healthcare/Radioterapia/data/ciolaplata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LhWAVjltIJWh"
   },
   "outputs": [],
   "source": [
    "# To support ResNet18 and ResNet34 for tensorflow.keras\n",
    "from classification_models.tfkeras import Classifiers\n",
    "from util.preprocess import rescale_0_to_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uFuIiQp2zVUF"
   },
   "outputs": [],
   "source": [
    "# ===============================================DEFINE YOUR ARGUMENTS=================================================\n",
    "if not isGoogleColab():\n",
    "    ARG_DATASET_DIR='/hdd/data/radioterapia/ciolaplata'\n",
    "# folder under ARG_DATASET_DIR path.\n",
    "ARG_RANDOM_SEED = 23456\n",
    "# ARG_TFDATASET_FOLDER=f'tfds.2018-2019.localnorm.DS10%.{ARG_RANDOM_SEED}.fold0'\n",
    "ARG_TFDATASET_FOLDER=f'tfds.2017.localnorm.DS10%.{ARG_RANDOM_SEED}.fold4'\n",
    "# if False only training and validation partition are created.\n",
    "ARG_TEST_PARTITION=False\n",
    "# number of continuous epochs without improvement on validation\n",
    "ARG_EPOCHS_WO_IMPROVEMENT=20 # 20 for 2019 only, 10 for 2019+2018\n",
    "initial_epochs = 10 # 10 for 2019 only, 5 for 2019+2018\n",
    "# maximum fine-tuning epochs\n",
    "ARG_MAX_FINE_TUNING_EPOCHS=150\n",
    "ARG_RESNET_NETWORK=True\n",
    "ARG_DATA_AUGMENTATION=False\n",
    "# perform data augmentation of images with a gamma value lower than gamma_augment\n",
    "ARG_GAMMA_AUGMENT=97.0\n",
    "# set this value based on the ARG_OVERSAMPLING_FACTOR value in tf2_oversampling_dicom_files.py\n",
    "# 1.0 means transform each and every image, with a lower value than 1.0 means that training will include unmodified (not transformed)\n",
    "# images.\n",
    "ARG_AUGMENT_PROBABILITY=0.75 # con 0.85 para el dataset tfds.2019.localnorm.ovs95x8.0 no transformo 93*(1+8)*0.15=125 imágenes  \n",
    "add_regularizers=False\n",
    "## Fine-tune from this layer onwards\n",
    "# fine_tune_at = 281 # InceptionV3, fine-tuning\n",
    "# fine_tune_at = 102 # InceptionV3, not so fine-tuning\n",
    "# fine_tune_at = 12 # VGG16\n",
    "fine_tune_at = 47 # resnet18 stage3\n",
    "# fine_tune_at = 74 # resnet34 stage3\n",
    "# fine_tune_at = 129 # resnet34 stage4\n",
    "ARG_TRANSFORM_GAMMA=False\n",
    "# 0: use custom LR, 1: use ReduceLROnPlateau\n",
    "ARG_LR_SCHEDULE=1\n",
    "ARG_LR_PATIENCE=10 # only applies if ARG_LR_SCHEDULE is 1.\n",
    "ARG_MIN_DELTA_MAE=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v77rlkCKW0IJ"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "### Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KVh7rDVAuW8Y"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "tf.random.set_seed(ARG_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1KR9xb8NyFTC"
   },
   "outputs": [],
   "source": [
    "def _tfrecord_dataset_type_from_folder(folder, dataset_type, ext='.tfrecords'):\n",
    "    tfrecords = [os.path.join(folder, n)\n",
    "                 for n in os.listdir(folder) if n.startswith(dataset_type) and n.endswith(ext)]\n",
    "    return tf.data.TFRecordDataset(tfrecords)\n",
    "\n",
    "tfdataset_dir = os.path.join(ARG_DATASET_DIR, ARG_TFDATASET_FOLDER)\n",
    "raw_train = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'train')\n",
    "raw_validation = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'validation')\n",
    "if ARG_TEST_PARTITION:\n",
    "    raw_test = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o29EfE-p0g5X"
   },
   "source": [
    "The resulting `tf.data.Dataset` objects contain `(image, label)` pairs where the images have variable shape and 3 channels, and the label is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GIys1_zY1S9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TFRecordDatasetV2 shapes: (), types: tf.string>\n",
      "<TFRecordDatasetV2 shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "if ARG_TEST_PARTITION:\n",
    "    print(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T1Gm1wdHyFTK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in validation part = 610.\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of images in validation part = {sum(1 for _ in raw_validation)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPyydf8h-ayL"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL6NwT3O-kXL"
   },
   "source": [
    "### Center Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AKkkw09v-MiA"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def crop(im, r, c, target_r, target_c): return im[r:r+target_r, c:c+target_c]\n",
    "\n",
    "# random crop to the original size\n",
    "def random_crop(x, r_pix=8):\n",
    "    \"\"\" Returns a random crop\"\"\"\n",
    "    r, c,*_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    start_r = np.floor(2*rand_r*r_pix).astype(int)\n",
    "    start_c = np.floor(2*rand_c*c_pix).astype(int)\n",
    "    return crop(x, start_r, start_c, r-2*r_pix, c-2*c_pix)\n",
    "\n",
    "def center_crop(x, r_pix=8):\n",
    "    r, c,*_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    return crop(x, r_pix, c_pix, r-2*r_pix, c-2*c_pix)\n",
    "\n",
    "\n",
    "def rotate_cv(im, deg, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n",
    "    \"\"\" Rotates an image by deg degrees\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    print(f\"rotate_cv: {r},{c}\")\n",
    "    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n",
    "    return cv2.warpAffine(im,M,(c,r), borderMode=mode, \n",
    "                          flags=cv2.WARP_FILL_OUTLIERS+interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d56BNppM-tOp"
   },
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lOmtnOTa-1Sy"
   },
   "outputs": [],
   "source": [
    "def random_translation(x, t_pix=16):\n",
    "    \"\"\" Returns a random translation\"\"\"\n",
    "    rows, cols, *_ = x.shape\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    t_r = np.floor(rand_r*t_pix).astype(int)\n",
    "    t_c = np.floor(rand_c*t_pix).astype(int)\n",
    "    # transformation T does shift (t_r, t_c)\n",
    "    # [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the output point\n",
    "    # (x, y) to a transformed input point\n",
    "    # (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k),\n",
    "    # where k = c0 x + c1 y + 1\n",
    "    T = [1, 0, t_r, 0, 1, t_c, 0, 0]\n",
    "    return tfa.image.transform (x, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ar8TNzTSyFTS"
   },
   "outputs": [],
   "source": [
    "def _parse_image_function(example_proto, img_size, normalization_fn, data_augmentation=False, augment_probability=1.0, transform_gamma=False):\n",
    "    # Create a dictionary describing the features.\n",
    "    image_feature_description = {\"image/filename\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/encoded\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/format\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/gamma_index\": tf.io.FixedLenFeature((), tf.float32),\n",
    "                \"image/height\": tf.io.FixedLenFeature((), tf.int64),\n",
    "                \"image/width\": tf.io.FixedLenFeature((), tf.int64)}\n",
    "    \n",
    "    def image_augment(image):\n",
    "        radian = ((np.random.random()-.50)*10 / 360) * np.pi\n",
    "        # tfa.image.transform_ops.rotate(image, radian) is an alias\n",
    "        image = tfa.image.rotate(image, radian)\n",
    "        # image = tf.image.random_flip_left_right(image)\n",
    "        image = random_translation(image)\n",
    "        return image\n",
    "\n",
    "    # Now, globally set everything to run eagerly\n",
    "    # The following doesn't set to eager mode:\n",
    "    # UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set,\n",
    "    # this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
    "    # tf.config.run_functions_eagerly(True)\n",
    "\n",
    "    # Executing eagerly = False here!\n",
    "    # numpy is only supported in eager mode. If you are in graph mode, it will not be supported.\n",
    "    # In eager execution the shape is always fully-known.\n",
    "    # print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    \n",
    "    image = tf.image.decode_jpeg(parsed[\"image/encoded\"], channels=3)\n",
    "    # print(type(image), image.shape, image.dtype) # <class 'tensorflow.python.framework.ops.Tensor'> (None, None, 3) <dtype: 'uint8'>\n",
    "\n",
    "    gamma = tf.cast(\n",
    "        parsed[\"image/gamma_index\"],\n",
    "        tf.float32)\n",
    "    # print(type(gamma), gamma.shape, gamma.dtype) # <class 'tensorflow.python.framework.ops.Tensor'> () <dtype: 'float32'>\n",
    "\n",
    "    image = normalization_fn(image)\n",
    "\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    print(type(image), image.shape, image.dtype)\n",
    "\n",
    "    if data_augmentation and augment_probability >= random.uniform(0, 1):\n",
    "        gamma_augment = tf.constant(ARG_GAMMA_AUGMENT)\n",
    "        image = tf.cond(tf.math.less(gamma, gamma_augment)\n",
    "                        , lambda: image_augment(image)\n",
    "                        , lambda: image)\n",
    "\n",
    "    # normalizo antes de transformar\n",
    "    # image = normalization_fn(image)\n",
    "    \n",
    "    #label is a tensor of an array of single tf.int64 arrays.\n",
    "    #label = tf.cast(\n",
    "    #    tf.reshape(parsed[\"image/class/label\"], [-1]),\n",
    "    #    tf.int64)\n",
    "\n",
    "    # assert tf.executing_eagerly() FAILS\n",
    "    # parsed[\"image/filename\"] is a Tensor and not an EagerTensor because we are in a map function,\n",
    "    # because in 2.0, code inside Datasets maps is turned into a subgraph for speed, just as it was in 1.x eager\n",
    "    # execution. You generally want to avoid Python inside your data pipeline.\n",
    "    # So, if I invoke parsed[\"image/filename\"].numpy().decode('utf-8') to get the filename string the error\n",
    "    # \"AttributeError: 'Tensor' object has no attribute 'numpy'\" is thrown, hence I return a tensor.\n",
    "    filename = parsed[\"image/filename\"]\n",
    "\n",
    "    if transform_gamma:\n",
    "        gamma = 60.0 - (105 - gamma)\n",
    "                    \n",
    "    return image, gamma, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CyTNIMaeyFTX"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 256 # All images will be resized to 256x256\n",
    "normalization_fn = rescale_0_to_1 # rescale_min_1_to_1\n",
    "# normalization_fn = tf.image.per_image_standardization # loss y mae en validación reportan números muy grandes,\n",
    "# no así en training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SFZ6ZW7KSXP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'> (256, 256, 3) <dtype: 'float32'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> (256, 256, 3) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "num_workers = 8\n",
    "\n",
    "# assert tf.executing_eagerly()\n",
    "if ARG_DATA_AUGMENTATION:\n",
    "    print(\"Training with image augmentation.\")\n",
    "    \n",
    "train = raw_train.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn, ARG_DATA_AUGMENTATION, ARG_AUGMENT_PROBABILITY,\n",
    "                                                      transform_gamma=ARG_TRANSFORM_GAMMA),\n",
    "                      num_parallel_calls=num_workers)\n",
    "validation = raw_validation.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn, transform_gamma=ARG_TRANSFORM_GAMMA),\n",
    "                                num_parallel_calls=num_workers)\n",
    "if ARG_TEST_PARTITION:\n",
    "    test = raw_test.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn),\n",
    "                        num_parallel_calls=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Yic-I66m6Isv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "PREFETCH_BUFFER_SIZE = 2 * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "p3UUPdm86LNC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing eagerly = True\n",
      "<PrefetchDataset shapes: ((None, 256, 256, 3), (None,), (None,)), types: (tf.float32, tf.float32, tf.string)>\n",
      "<PrefetchDataset shapes: ((None, 256, 256, 3), (None,), (None,)), types: (tf.float32, tf.float32, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "\n",
    "# <PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.int64)>\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER_SIZE)\n",
    "print(train_batches)\n",
    "validation_batches = validation.batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER_SIZE)\n",
    "print(validation_batches)\n",
    "# <BatchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.int64)>\n",
    "if ARG_TEST_PARTITION:\n",
    "    test_batches = test.batch(BATCH_SIZE)\n",
    "    print(test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02rJpcFtChP0"
   },
   "source": [
    "Inspect a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iknFo3ELBVho"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 256, 256, 3]), TensorShape([32]), TensorShape([32]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch, filename_batch in train_batches.take(1):\n",
    "    pass\n",
    "\n",
    "image_batch.shape, label_batch.shape, filename_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5pIy5ehM4YzC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=96.48, filename=/hdd/data/radioterapia/ciolaplata/2017/1.3.6.1.4.1.9590.100.1.2.57836708811090489602588159133155207606.dcm\n",
      "image shape = (256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display 2nd image in the batch\n",
    "gamma_value = label_batch[8].numpy()\n",
    "filename = filename_batch[8].numpy().decode('utf-8')\n",
    "print(f'gamma={gamma_value:2.2f}, filename={filename}')\n",
    "# if pixel values are float they have to be in [0, 1] range, if they are integer they have to be in the [0, 255] range,\n",
    "# else pixel values are truncated.\n",
    "im = image_batch[8].numpy()\n",
    "plt.imshow(im)\n",
    "print(\"image shape = {}\".format(im.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im[:,:,0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkH-kazQecHB"
   },
   "source": [
    "## Create the base model from the pre-trained convnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aiLcQ7JkDM6U"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "if ARG_RESNET_NETWORK:\n",
    "    ResNet18, preprocess_input = Classifiers.get('resnet18')\n",
    "    base_model = ResNet18(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)\n",
    "\n",
    "else:\n",
    "    ## Create the base model from the pre-trained model MobileNet V2\n",
    "    #base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "    #                                               # alpha=1.4,\n",
    "    #                                               include_top=False,\n",
    "    #      \n",
    "\n",
    "    \n",
    "    # Create the base model from the pre-trained model Inception V3\n",
    "\n",
    "    # When include_top=True and weights=None (random initialization), see below:\n",
    "    # __________________________________________________________________________________________________\n",
    "    # avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
    "    # __________________________________________________________________________________________________\n",
    "    # predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
    "    # ==================================================================================================        \n",
    "\n",
    "    #base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "    #                                               weights='imagenet',\n",
    "    #                                               include_top=False)\n",
    "\n",
    "    #base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "    # \n",
    "\n",
    "\n",
    "    base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                                   weights='imagenet',\n",
    "                                                   include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcsxoJIEVXZ"
   },
   "source": [
    "This feature extractor converts each `160x160x3` image into a `5x5x1280` block of features. See what it does to the example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Y-2LJL0EEUcx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 8, 512)\n"
     ]
    }
   ],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlx56nQtfe8Y"
   },
   "source": [
    "## Feature extraction\n",
    "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnMLieHBCwil"
   },
   "source": [
    "### Freeze the convolutional base\n",
    "\n",
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OTCJH4bphOeo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn_data (BatchNormalization)\n",
      "bn0 (BatchNormalization)\n",
      "stage1_unit1_bn1 (BatchNormalization)\n",
      "stage1_unit1_bn2 (BatchNormalization)\n",
      "stage1_unit2_bn1 (BatchNormalization)\n",
      "stage1_unit2_bn2 (BatchNormalization)\n",
      "stage2_unit1_bn1 (BatchNormalization)\n",
      "stage2_unit1_bn2 (BatchNormalization)\n",
      "stage2_unit2_bn1 (BatchNormalization)\n",
      "stage2_unit2_bn2 (BatchNormalization)\n",
      "stage3_unit1_bn1 (BatchNormalization)\n",
      "stage3_unit1_bn2 (BatchNormalization)\n",
      "stage3_unit2_bn1 (BatchNormalization)\n",
      "stage3_unit2_bn2 (BatchNormalization)\n",
      "stage4_unit1_bn1 (BatchNormalization)\n",
      "stage4_unit1_bn2 (BatchNormalization)\n",
      "stage4_unit2_bn1 (BatchNormalization)\n",
      "stage4_unit2_bn2 (BatchNormalization)\n",
      "bn1 (BatchNormalization)\n"
     ]
    }
   ],
   "source": [
    "if not ARG_RESNET_NETWORK:\n",
    "    base_model.trainable = False\n",
    "else:\n",
    "    # resnet\n",
    "    for layer in base_model.layers:\n",
    "            if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "                layer.trainable = False\n",
    "            else:\n",
    "                print(f\"{layer.name} ({layer.__class__.__name__})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KpbzSmPkDa-N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 256, 256, 3)  9           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 262, 262, 3)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 128, 128, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 128, 128, 64) 256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 128, 128, 64) 0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 130, 130, 64) 0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 64, 64, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 64, 64, 64)   256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 64, 64, 64)   0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 64, 64, 64)   256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 64, 64, 64)   0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 64, 64, 64)   4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 64, 64)   0           stage1_unit1_conv2[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 64, 64, 64)   256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 64, 64, 64)   0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 64, 64, 64)   256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 64, 64, 64)   0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 64)   0           stage1_unit2_conv2[0][0]         \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 64, 64, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 64, 64, 64)   0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 66, 66, 64)   0           stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 32, 32, 128)  73728       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 32, 32, 128)  512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 32, 32, 128)  0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 32, 32, 128)  8192        stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 128)  0           stage2_unit1_conv2[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 32, 32, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 32, 32, 128)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 32, 32, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 32, 32, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 128)  0           stage2_unit2_conv2[0][0]         \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 32, 32, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 32, 32, 128)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 34, 34, 128)  0           stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 16, 16, 256)  294912      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 16, 16, 256)  1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 16, 16, 256)  0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 16, 16, 256)  32768       stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 256)  0           stage3_unit1_conv2[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 16, 16, 256)  1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 16, 16, 256)  0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 16, 16, 256)  1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 16, 16, 256)  0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 256)  0           stage3_unit2_conv2[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 16, 16, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 16, 16, 256)  0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 18, 18, 256)  0           stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 8, 8, 512)    1179648     zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 8, 8, 512)    2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 8, 8, 512)    0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 8, 8, 512)    131072      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           stage4_unit1_conv2[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 8, 8, 512)    2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 8, 8, 512)    0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 8, 8, 512)    2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 8, 8, 512)    0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 512)    0           stage4_unit2_conv2[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 8, 8, 512)    2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 8, 8, 512)    0           bn1[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 11,186,889\n",
      "Trainable params: 7,939\n",
      "Non-trainable params: 11,178,950\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7RFyBW06yFUC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/fluence_maps'\n",
      "/home/pablo/dev/radioterapia/fluence_maps\n",
      "layer conv0 has no regularizer.\n",
      "layer stage1_unit1_conv1 has no regularizer.\n",
      "layer stage1_unit1_conv2 has no regularizer.\n",
      "layer stage1_unit1_sc has no regularizer.\n",
      "layer stage1_unit2_conv1 has no regularizer.\n",
      "layer stage1_unit2_conv2 has no regularizer.\n",
      "layer stage2_unit1_conv1 has no regularizer.\n",
      "layer stage2_unit1_conv2 has no regularizer.\n",
      "layer stage2_unit1_sc has no regularizer.\n",
      "layer stage2_unit2_conv1 has no regularizer.\n",
      "layer stage2_unit2_conv2 has no regularizer.\n",
      "layer stage3_unit1_conv1 has no regularizer.\n",
      "layer stage3_unit1_conv2 has no regularizer.\n",
      "layer stage3_unit1_sc has no regularizer.\n",
      "layer stage3_unit2_conv1 has no regularizer.\n",
      "layer stage3_unit2_conv2 has no regularizer.\n",
      "layer stage4_unit1_conv1 has no regularizer.\n",
      "layer stage4_unit1_conv2 has no regularizer.\n",
      "layer stage4_unit1_sc has no regularizer.\n",
      "layer stage4_unit2_conv1 has no regularizer.\n",
      "layer stage4_unit2_conv2 has no regularizer.\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/fluence_maps'\n",
    "from add_regularization import add_regularization\n",
    "# adds a tf.keras.regularizers.l2(0.0001)\n",
    "#\"kernel_regularizer\":{\n",
    "#                        \"class_name\": \"L1L2\",\n",
    "#                        \"config\": {\n",
    "#                            \"l1\": 0,\n",
    "#                            \"l2\": 0.0001\n",
    "#                        }\n",
    "#                     }\n",
    "if add_regularizers:\n",
    "    base_model = add_regularization(base_model)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    for attr in ['kernel_regularizer']:\n",
    "        if hasattr(layer, attr):\n",
    "            if getattr(layer, attr) is None:\n",
    "                print('layer {} has no regularizer.'.format(layer.name))\n",
    "            else:\n",
    "                print('layer {} has a regularizer {}.'.format(layer.name, getattr(layer, attr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WKjwYuIryFUE"
   },
   "outputs": [],
   "source": [
    "# from render_json import RenderJSON\n",
    "\n",
    "# RenderJSON(base_model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "avX9AnbJyFUH"
   },
   "outputs": [],
   "source": [
    "# display the weights of some layers\n",
    "l = 1\n",
    "for layer in base_model.layers:\n",
    "    # print('layer {:3d}, name: {}'.format(l, layer.name))\n",
    "    if layer.name == \"expanded_conv_project\":\n",
    "        weights = layer.get_weights()\n",
    "        print(layer.get_config(), weights, weights[0].shape)\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dLnpMF5KOALm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 512)\n"
     ]
    }
   ],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzmSozfKDM6W"
   },
   "source": [
    "Stack the feature extractor, and these two layers using a `tf.keras.Sequential` model for network architectures other than ResNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Wv4afXKj6cVa"
   },
   "outputs": [],
   "source": [
    "if not ARG_RESNET_NETWORK:\n",
    "    # Pablo March 10: add sigmoid\n",
    "    # WARNING: adding the activation function causes loss to keep close to 0.5 and does not decrease.\n",
    "    # prediction_layer = keras.layers.Dense(1, activation='sigmoid') # para obtener probabilidades y no logits\n",
    "\n",
    "    # https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes\n",
    "    # https://stackoverflow.com/questions/58627411/how-to-use-inception-network-for-regression\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    fc1 = keras.layers.Dense(512, activation='relu')\n",
    "    #fc2 = keras.layers.Dense(512, activation='relu')\n",
    "    #dropout = keras.layers.Dropout(rate=0.05) # no funciona\n",
    "    # and a linear output layer (regression)\n",
    "\n",
    "    bn = keras.layers.BatchNormalization() # same as ResNet18 but with VGG16 is worst.\n",
    "\n",
    "    prediction_layer = keras.layers.Dense(1, activation='linear')\n",
    "    # and a logistic layer -- let's say we have 200 classes (classification)\n",
    "    # prediction_layer = Dense(200, activation='softmax')(x)\n",
    "\n",
    "    # Up to March 27, 2020\n",
    "    # prediction_layer = keras.layers.Dense(1)\n",
    "    prediction_batch = prediction_layer(feature_batch_average)\n",
    "    print(prediction_batch.shape)\n",
    "\n",
    "    # Now stack the feature extractor, and these two layers using a `tf.keras.Sequential` model:\n",
    "    model = tf.keras.Sequential([\n",
    "      base_model,\n",
    "      global_average_layer,\n",
    "      fc1,\n",
    "      #bn,\n",
    "      #dropout,\n",
    "      #fc2,\n",
    "      prediction_layer\n",
    "    ])\n",
    "    \n",
    "else:\n",
    "    avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    out = tf.keras.layers.Dense(512, activation=\"relu\")(avg) # CHANGE to use avg pool only or avg pool + max pool\n",
    "    # out = tf.keras.layers.BatchNormalization()(out) # no observé mejoras en 2017 agregando esta bn layer.\n",
    "    prediction_layer = tf.keras.layers.Dense(1, activation='linear')(out)\n",
    "    model = keras.models.Model(inputs=base_model.input, outputs=prediction_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "You must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss with `from_logits=True` since the model provides a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RpR8HdyMhukJ"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "# mse = square(y_true - y_pred)\n",
    "# mae = loss = abs(y_true - y_pred)\n",
    "# mape = 100 * abs(y_true - y_pred) / y_true\n",
    "# mae y mape son similares, no iguales, por eso tomo MAE que es el promedio de la diferencia absoluta entre el\n",
    "# gamma real y el gamma predicho\n",
    "#optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate, momentum=0.95)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "I8ARiyMFsgbH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 256, 256, 3)  9           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 262, 262, 3)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 128, 128, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 128, 128, 64) 256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 128, 128, 64) 0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 130, 130, 64) 0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 64, 64, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 64, 64, 64)   256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 64, 64, 64)   0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 64, 64, 64)   256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 64, 64, 64)   0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 64, 64, 64)   4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 64, 64)   0           stage1_unit1_conv2[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 64, 64, 64)   256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 64, 64, 64)   0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 64, 64, 64)   256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 64, 64, 64)   0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 64)   0           stage1_unit2_conv2[0][0]         \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 64, 64, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 64, 64, 64)   0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 66, 66, 64)   0           stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 32, 32, 128)  73728       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 32, 32, 128)  512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 32, 32, 128)  0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 32, 32, 128)  8192        stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 128)  0           stage2_unit1_conv2[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 32, 32, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 32, 32, 128)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 32, 32, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 32, 32, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 128)  0           stage2_unit2_conv2[0][0]         \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 32, 32, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 32, 32, 128)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 34, 34, 128)  0           stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 16, 16, 256)  294912      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 16, 16, 256)  1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 16, 16, 256)  0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 16, 16, 256)  32768       stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 256)  0           stage3_unit1_conv2[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 16, 16, 256)  1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 16, 16, 256)  0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 16, 16, 256)  1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 16, 16, 256)  0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 256)  0           stage3_unit2_conv2[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 16, 16, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 16, 16, 256)  0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 18, 18, 256)  0           stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 8, 8, 512)    1179648     zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 8, 8, 512)    2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 8, 8, 512)    0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 8, 8, 512)    131072      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           stage4_unit1_conv2[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 8, 8, 512)    2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 8, 8, 512)    0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 8, 8, 512)    2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 8, 8, 512)    0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 512)    0           stage4_unit2_conv2[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 8, 8, 512)    2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 8, 8, 512)    0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          262656      global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 11,450,058\n",
      "Trainable params: 271,108\n",
      "Non-trainable params: 11,178,950\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hlHEavK7DUI7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train partition: 2443, in validation: 610.\n"
     ]
    }
   ],
   "source": [
    "num_train = sum(1 for _ in raw_train)\n",
    "num_val = sum(1 for _ in raw_validation)\n",
    "print(f'Number of images in train partition: {num_train}, in validation: {num_val}.')\n",
    "if ARG_TEST_PARTITION:\n",
    "    num_test = sum(1 for _ in raw_test)\n",
    "    print(f'Number of images in test partiton: {num_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Om4O3EESkab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.float32)>\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 95.7996 - mse: 9201.4648\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = round(num_train)//BATCH_SIZE\n",
    "validation_steps=20\n",
    "\n",
    "# projects out just the first two components.\n",
    "tmp_validation_batches = validation_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "print(tmp_validation_batches)\n",
    "\n",
    "loss0 = mse0 = 0\n",
    "loss0, mse0 = model.evaluate(tmp_validation_batches, steps = validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8cYT1c48CuSd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 95.80\n",
      "initial mape: 9201.46\n"
     ]
    }
   ],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial mape: {:.2f}\".format(mse0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "JsaRFlZ9B6WK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "# projects out just the first two components.\n",
    "tmp_train_batches = train_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "print(tmp_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XTtp6LG7yFUw"
   },
   "outputs": [],
   "source": [
    "# Implement callback function to stop training\n",
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, wait_epochs):\n",
    "        self.__wait_epochs = wait_epochs\n",
    "        self.__latest_peak_mae = 9.0\n",
    "        self.__waited_epochs = 0\n",
    "        self.__saved_model_file = None\n",
    "        self.__saved_model = None\n",
    "        \n",
    "    def stopTraining(self, epoch, val_mae):\n",
    "        stop_early = False\n",
    "        best_mae = False\n",
    "        # check for early stop\n",
    "        if val_mae < self.__latest_peak_mae and abs(val_mae - self.__latest_peak_mae) > ARG_MIN_DELTA_MAE:\n",
    "            best_mae = True\n",
    "            self.__latest_peak_mae = val_mae\n",
    "            print(f\"\\nNew peak val_mae reached: {val_mae:6.3}\")\n",
    "\n",
    "            t = time.time()\n",
    "            dir = os.path.join(ARG_DATASET_DIR, \"models\")\n",
    "            save_model_path = \"{}/{}.{}.h5\".format(dir, int(t), ARG_RANDOM_SEED)\n",
    "            print(save_model_path)\n",
    "            # Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' \n",
    "            # in TF 2.X, and 'h5' in TF 1.X.            \n",
    "            model.save(save_model_path, save_format='h5')\n",
    "            # borro el archivo del modelo anterior\n",
    "            if self.__saved_model_file is not None:\n",
    "                os.remove(self.__saved_model_file)\n",
    "            self.__saved_model_file = save_model_path\n",
    "            reloaded_model = tf.keras.models.load_model(save_model_path)\n",
    "            self.__saved_model = reloaded_model\n",
    "\n",
    "        if val_mae > self.__latest_peak_mae:\n",
    "            # Si llevo N+ epochs sin mejora\n",
    "            if self.__waited_epochs >= self.__wait_epochs:\n",
    "                print(\"\\nStopping early at epoch {0} with saved peak mae {1:10.8}\"\n",
    "                      .format(epoch + 1, self.__latest_peak_mae))\n",
    "                stop_early = True\n",
    "            \n",
    "            self.__waited_epochs += 1\n",
    "            \n",
    "        elif best_mae:\n",
    "            self.__latest_peak_mae = val_mae\n",
    "            # Reset waited epochs.\n",
    "            print(\"waiting epochs reset.\")\n",
    "            self.reset_waited_epochs()\n",
    "            \n",
    "        return stop_early\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # print('\\nTraining: epoch {} ends at {}'.format(epoch, datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "        if self.stopTraining(epoch, logs.get('val_loss')):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    @property\n",
    "    def saved_model_file(self):\n",
    "        return self.__saved_model_file\n",
    "    \n",
    "    @property\n",
    "    def saved_model(self):\n",
    "        return self.__saved_model\n",
    "\n",
    "    def reset_waited_epochs(self):\n",
    "        self.__waited_epochs = 0;\n",
    "    \n",
    "\n",
    "# Instantiate a callback object\n",
    "callbackObj = MyCallback(ARG_EPOCHS_WO_IMPROVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "MZyPn887yFUz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 16.4566 - mse: 822.6501 - val_loss: 17.9127 - val_mse: 344.1534\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 3.7467 - mse: 24.7857 - val_loss: 20.2940 - val_mse: 435.5658\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 3.1231 - mse: 17.7969 - val_loss: 35.8471 - val_mse: 1310.8357\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 2.8601 - mse: 15.5005 - val_loss: 17.6896 - val_mse: 335.4272\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 2.8502 - mse: 14.9132 - val_loss: 12.2479 - val_mse: 167.4278\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 2.6402 - mse: 13.1015 - val_loss: 10.8883 - val_mse: 146.4712\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 2.5038 - mse: 11.7378 - val_loss: 5.6649 - val_mse: 45.2581\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 2.4991 - mse: 11.3993 - val_loss: 14.2287 - val_mse: 247.6903\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 2.2513 - mse: 9.9710 - val_loss: 4.7003 - val_mse: 37.3035\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 2.3919 - mse: 10.5201 - val_loss: 4.3751 - val_mse: 34.4920\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tmp_train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=tmp_validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd94CKImf8vi"
   },
   "source": [
    "### Learning curves\n",
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNet V2 base model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zvmYWOgoyFU4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "4Po3M6UrGvKr"
   },
   "outputs": [],
   "source": [
    "# Pablo Feb 25:\n",
    "# Python never implicitly copies objects. When you set list2 = list1, you are making them refer to the same exact\n",
    "# list object, so when you mutate it, all references to it keep referring to the object in its current state.\n",
    "mse = history.history['mse'].copy()\n",
    "val_mse = history.history['val_mse'].copy()\n",
    "\n",
    "loss = history.history['loss'].copy()\n",
    "val_loss = history.history['val_loss'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "53OTCh3jnbwV"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(mse, label='Training MSE')\n",
    "plt.plot(val_mse, label='Validation MSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.ylim([0,50])\n",
    "plt.title('Training and Validation MSE')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0,10])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqwV-CRdS6Nv"
   },
   "source": [
    "## Fine tuning\n",
    "In the feature extraction experiment, you were only training a few layers on top of a VGG16 base model. The weights of the pre-trained network were **not** updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n",
    "\n",
    "Note: This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned.\n",
    "\n",
    "Also, you should try to fine-tune a small number of top layers rather than the whole VGG16 model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPXnzUK0QonF"
   },
   "source": [
    "### Un-freeze the top layers of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfxv_ifotQak"
   },
   "source": [
    "All you need to do is unfreeze the `base_model` and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "4nzcagVitLQm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer   1, name: data\n",
      "layer   2, name: bn_data\n",
      "layer   3, name: zero_padding2d\n",
      "layer   4, name: conv0\n",
      "layer   5, name: bn0\n",
      "layer   6, name: relu0\n",
      "layer   7, name: zero_padding2d_1\n",
      "layer   8, name: pooling0\n",
      "layer   9, name: stage1_unit1_bn1\n",
      "layer  10, name: stage1_unit1_relu1\n",
      "layer  11, name: zero_padding2d_2\n",
      "layer  12, name: stage1_unit1_conv1\n",
      "layer  13, name: stage1_unit1_bn2\n",
      "layer  14, name: stage1_unit1_relu2\n",
      "layer  15, name: zero_padding2d_3\n",
      "layer  16, name: stage1_unit1_conv2\n",
      "layer  17, name: stage1_unit1_sc\n",
      "layer  18, name: add\n",
      "layer  19, name: stage1_unit2_bn1\n",
      "layer  20, name: stage1_unit2_relu1\n",
      "layer  21, name: zero_padding2d_4\n",
      "layer  22, name: stage1_unit2_conv1\n",
      "layer  23, name: stage1_unit2_bn2\n",
      "layer  24, name: stage1_unit2_relu2\n",
      "layer  25, name: zero_padding2d_5\n",
      "layer  26, name: stage1_unit2_conv2\n",
      "layer  27, name: add_1\n",
      "layer  28, name: stage2_unit1_bn1\n",
      "layer  29, name: stage2_unit1_relu1\n",
      "layer  30, name: zero_padding2d_6\n",
      "layer  31, name: stage2_unit1_conv1\n",
      "layer  32, name: stage2_unit1_bn2\n",
      "layer  33, name: stage2_unit1_relu2\n",
      "layer  34, name: zero_padding2d_7\n",
      "layer  35, name: stage2_unit1_conv2\n",
      "layer  36, name: stage2_unit1_sc\n",
      "layer  37, name: add_2\n",
      "layer  38, name: stage2_unit2_bn1\n",
      "layer  39, name: stage2_unit2_relu1\n",
      "layer  40, name: zero_padding2d_8\n",
      "layer  41, name: stage2_unit2_conv1\n",
      "layer  42, name: stage2_unit2_bn2\n",
      "layer  43, name: stage2_unit2_relu2\n",
      "layer  44, name: zero_padding2d_9\n",
      "layer  45, name: stage2_unit2_conv2\n",
      "layer  46, name: add_3\n",
      "layer  47, name: stage3_unit1_bn1\n",
      "layer  48, name: stage3_unit1_relu1\n",
      "layer  49, name: zero_padding2d_10\n",
      "layer  50, name: stage3_unit1_conv1\n",
      "layer  51, name: stage3_unit1_bn2\n",
      "layer  52, name: stage3_unit1_relu2\n",
      "layer  53, name: zero_padding2d_11\n",
      "layer  54, name: stage3_unit1_conv2\n",
      "layer  55, name: stage3_unit1_sc\n",
      "layer  56, name: add_4\n",
      "layer  57, name: stage3_unit2_bn1\n",
      "layer  58, name: stage3_unit2_relu1\n",
      "layer  59, name: zero_padding2d_12\n",
      "layer  60, name: stage3_unit2_conv1\n",
      "layer  61, name: stage3_unit2_bn2\n",
      "layer  62, name: stage3_unit2_relu2\n",
      "layer  63, name: zero_padding2d_13\n",
      "layer  64, name: stage3_unit2_conv2\n",
      "layer  65, name: add_5\n",
      "layer  66, name: stage4_unit1_bn1\n",
      "layer  67, name: stage4_unit1_relu1\n",
      "layer  68, name: zero_padding2d_14\n",
      "layer  69, name: stage4_unit1_conv1\n",
      "layer  70, name: stage4_unit1_bn2\n",
      "layer  71, name: stage4_unit1_relu2\n",
      "layer  72, name: zero_padding2d_15\n",
      "layer  73, name: stage4_unit1_conv2\n",
      "layer  74, name: stage4_unit1_sc\n",
      "layer  75, name: add_6\n",
      "layer  76, name: stage4_unit2_bn1\n",
      "layer  77, name: stage4_unit2_relu1\n",
      "layer  78, name: zero_padding2d_16\n",
      "layer  79, name: stage4_unit2_conv1\n",
      "layer  80, name: stage4_unit2_bn2\n",
      "layer  81, name: stage4_unit2_relu2\n",
      "layer  82, name: zero_padding2d_17\n",
      "layer  83, name: stage4_unit2_conv2\n",
      "layer  84, name: add_7\n",
      "layer  85, name: bn1\n",
      "layer  86, name: relu1\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Pablo Feb 25: con este loop me aseguro que las capas que en la próxima celda *NO* pongo como layer.trainable = False sean\n",
    "# trainable.\n",
    "l = 1\n",
    "for layer in base_model.layers:\n",
    "    print('layer {:3d}, name: {}'.format(l, layer.name))\n",
    "    if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "        layer.trainable =  True\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-4HgVAacRs5v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  86\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Uk1dgsxT0IS"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Compile the model using a much lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "AVvlrXb9yFVH"
   },
   "outputs": [],
   "source": [
    "# This function keeps the learning rate at 0.001 for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def lr_scheduler(epoch):\n",
    "    lr = base_learning_rate\n",
    "    #k = 0.15\n",
    "    #if epoch >= 10 and epoch <= 40:\n",
    "    #    lr = tf.math.exp(k * (10 - epoch)) * base_learning_rate\n",
    "    #    print(\"\\nlearning rate: %.8f\"%(lr))\n",
    "    if epoch >= 10 and epoch <= 30:\n",
    "        lr = base_learning_rate/10\n",
    "    elif epoch > 30 and epoch <= 60:\n",
    "        lr = base_learning_rate/100\n",
    "    elif epoch > 60:\n",
    "        lr = base_learning_rate/500\n",
    "    print(\"\\nlearning rate: %.6f\"%(lr))\n",
    "    \n",
    "    return lr\n",
    "    \n",
    "    \n",
    "callbackLR = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "MvJPcR-PIEu2"
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=ARG_LR_PATIENCE, verbose=1, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "NtUnaz0WUDva"
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10, momentum=0.95)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10, epsilon=1e-08, amsgrad=False)\n",
    "model.compile(loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "              optimizer = optimizer,\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "WwBWy7J2kZvA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 256, 256, 3)  9           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 262, 262, 3)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 128, 128, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 128, 128, 64) 256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 128, 128, 64) 0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 130, 130, 64) 0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 64, 64, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 64, 64, 64)   256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 64, 64, 64)   0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 64, 64, 64)   256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 64, 64, 64)   0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 64, 64, 64)   4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 64, 64)   0           stage1_unit1_conv2[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 64, 64, 64)   256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 64, 64, 64)   0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 64, 64, 64)   256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 64, 64, 64)   0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 64)   0           stage1_unit2_conv2[0][0]         \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 64, 64, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 64, 64, 64)   0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 66, 66, 64)   0           stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 32, 32, 128)  73728       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 32, 32, 128)  512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 32, 32, 128)  0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 32, 32, 128)  8192        stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 128)  0           stage2_unit1_conv2[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 32, 32, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 32, 32, 128)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 32, 32, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 32, 32, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 128)  0           stage2_unit2_conv2[0][0]         \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 32, 32, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 32, 32, 128)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 34, 34, 128)  0           stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 16, 16, 256)  294912      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 16, 16, 256)  1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 16, 16, 256)  0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 16, 16, 256)  32768       stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 256)  0           stage3_unit1_conv2[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 16, 16, 256)  1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 16, 16, 256)  0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 16, 16, 256)  1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 16, 16, 256)  0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 256)  0           stage3_unit2_conv2[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 16, 16, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 16, 16, 256)  0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 18, 18, 256)  0           stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 8, 8, 512)    1179648     zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 8, 8, 512)    2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 8, 8, 512)    0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 8, 8, 512)    131072      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           stage4_unit1_conv2[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 8, 8, 512)    2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 8, 8, 512)    0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 8, 8, 512)    2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 8, 8, 512)    0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 512)    0           stage4_unit2_conv2[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 8, 8, 512)    2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 8, 8, 512)    0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          262656      global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 11,450,058\n",
      "Trainable params: 10,756,868\n",
      "Non-trainable params: 693,190\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "bNXelbMQtonr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G5O4jd6TuAG"
   },
   "source": [
    "### Continue training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "NKrhmlnv-4_M"
   },
   "outputs": [],
   "source": [
    "# Crear el path donde se va a ir guardando el mejor mejor modelo en training.\n",
    "#dirName = \"./tmp/saved_models/\"\n",
    "#if not os.path.exists(dirName):\n",
    "#    os.makedirs(dirName)\n",
    "#    print(\"Directory\" , dirName ,  \"Created.\")\n",
    "#else:    \n",
    "#    print(\"Directory\" , dirName ,  \"already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0foWUN-yDLo_"
   },
   "source": [
    "If you trained to convergence earlier, this step will improve your accuracy by a few percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECQLkAsFTlun"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/160\n",
      "     77/Unknown - 6s 74ms/step - loss: 2.8387 - mse: 15.2499\n",
      "New peak val_mae reached:   3.24\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466558.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 2.8387 - mse: 15.2499 - val_loss: 3.2368 - val_mse: 19.3346 - lr: 1.0000e-04\n",
      "Epoch 12/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 2.4280 - mse: 11.3901 - val_loss: 9.5179 - val_mse: 100.7939 - lr: 1.0000e-04\n",
      "Epoch 13/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 1.9728 - mse: 7.9164 - val_loss: 12.1499 - val_mse: 160.0798 - lr: 1.0000e-04\n",
      "Epoch 14/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 1.6629 - mse: 5.9447 - val_loss: 8.6449 - val_mse: 84.2423 - lr: 1.0000e-04\n",
      "Epoch 15/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 1.6574 - mse: 5.4576 - val_loss: 5.9596 - val_mse: 44.4155 - lr: 1.0000e-04\n",
      "Epoch 16/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.5838 - mse: 5.0757\n",
      "New peak val_mae reached:   2.38\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466591.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 1.5817 - mse: 5.0614 - val_loss: 2.3766 - val_mse: 10.2881 - lr: 1.0000e-04\n",
      "Epoch 17/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3488 - mse: 3.8411\n",
      "New peak val_mae reached:   2.02\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466599.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 1.3464 - mse: 3.8284 - val_loss: 2.0206 - val_mse: 8.3674 - lr: 1.0000e-04\n",
      "Epoch 18/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.3410 - mse: 3.4866\n",
      "New peak val_mae reached:   1.98\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466606.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 94ms/step - loss: 1.3393 - mse: 3.4792 - val_loss: 1.9768 - val_mse: 7.9390 - lr: 1.0000e-04\n",
      "Epoch 19/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 1.2322 - mse: 3.1952 - val_loss: 1.9716 - val_mse: 8.0034 - lr: 1.0000e-04\n",
      "Epoch 20/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.2098 - mse: 2.9526\n",
      "New peak val_mae reached:    1.8\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466620.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 1.2159 - mse: 2.9858 - val_loss: 1.8029 - val_mse: 7.3100 - lr: 1.0000e-04\n",
      "Epoch 21/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 1.2141 - mse: 2.8740 - val_loss: 2.9500 - val_mse: 13.8035 - lr: 1.0000e-04\n",
      "Epoch 22/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 1.0898 - mse: 2.2741 - val_loss: 2.2204 - val_mse: 9.4603 - lr: 1.0000e-04\n",
      "Epoch 23/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 1.0848 - mse: 2.3825 - val_loss: 2.0889 - val_mse: 10.5570 - lr: 1.0000e-04\n",
      "Epoch 24/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 1.2729 - mse: 2.9470 - val_loss: 2.0968 - val_mse: 9.2255 - lr: 1.0000e-04\n",
      "Epoch 25/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.0186 - mse: 2.0587\n",
      "New peak val_mae reached:   1.77\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466653.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 1.0187 - mse: 2.0593 - val_loss: 1.7718 - val_mse: 7.7974 - lr: 1.0000e-04\n",
      "Epoch 26/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.9732 - mse: 1.8077 - val_loss: 1.8788 - val_mse: 7.8038 - lr: 1.0000e-04\n",
      "Epoch 27/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.9557 - mse: 1.7499 - val_loss: 1.8396 - val_mse: 8.5255 - lr: 1.0000e-04\n",
      "Epoch 28/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.9706 - mse: 1.7432 - val_loss: 1.8821 - val_mse: 8.4382 - lr: 1.0000e-04\n",
      "Epoch 29/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 1.0272 - mse: 1.8925 - val_loss: 2.3654 - val_mse: 10.6425 - lr: 1.0000e-04\n",
      "Epoch 30/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.8843 - mse: 1.4788 - val_loss: 1.7802 - val_mse: 7.5639 - lr: 1.0000e-04\n",
      "Epoch 31/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.8690 - mse: 1.4151 - val_loss: 1.9544 - val_mse: 10.0240 - lr: 1.0000e-04\n",
      "Epoch 32/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.9188 - mse: 1.6119 - val_loss: 2.2647 - val_mse: 9.0552 - lr: 1.0000e-04\n",
      "Epoch 33/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.9076 - mse: 1.5512\n",
      "New peak val_mae reached:   1.76\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466706.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.9084 - mse: 1.5506 - val_loss: 1.7582 - val_mse: 7.8860 - lr: 1.0000e-04\n",
      "Epoch 34/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.8286 - mse: 1.3044 - val_loss: 1.8467 - val_mse: 8.8371 - lr: 1.0000e-04\n",
      "Epoch 35/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.9638 - mse: 1.6702 - val_loss: 1.8026 - val_mse: 7.4052 - lr: 1.0000e-04\n",
      "Epoch 36/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.7745 - mse: 1.1134 - val_loss: 2.6719 - val_mse: 12.6937 - lr: 1.0000e-04\n",
      "Epoch 37/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.8154 - mse: 1.1898\n",
      "New peak val_mae reached:   1.73\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466733.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.8157 - mse: 1.1909 - val_loss: 1.7302 - val_mse: 7.4520 - lr: 1.0000e-04\n",
      "Epoch 38/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.7683 - mse: 1.1207 - val_loss: 1.7755 - val_mse: 7.4900 - lr: 1.0000e-04\n",
      "Epoch 39/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.8387 - mse: 1.2842 - val_loss: 1.8461 - val_mse: 7.8905 - lr: 1.0000e-04\n",
      "Epoch 40/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.7624 - mse: 1.0987 - val_loss: 1.8506 - val_mse: 7.5072 - lr: 1.0000e-04\n",
      "Epoch 41/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.8361 - mse: 1.2264 - val_loss: 1.7543 - val_mse: 7.7856 - lr: 1.0000e-04\n",
      "Epoch 42/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.7418 - mse: 1.0418 - val_loss: 2.1008 - val_mse: 9.6420 - lr: 1.0000e-04\n",
      "Epoch 43/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.7712 - mse: 1.0782 - val_loss: 1.8775 - val_mse: 7.4030 - lr: 1.0000e-04\n",
      "Epoch 44/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.7918 - mse: 1.1065 - val_loss: 2.0489 - val_mse: 8.9105 - lr: 1.0000e-04\n",
      "Epoch 45/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.7466 - mse: 1.0105 - val_loss: 2.0747 - val_mse: 8.8097 - lr: 1.0000e-04\n",
      "Epoch 46/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.8062 - mse: 1.1385 - val_loss: 1.9673 - val_mse: 8.1560 - lr: 1.0000e-04\n",
      "Epoch 47/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.7719 - mse: 1.0900\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.7701 - mse: 1.0861 - val_loss: 1.7390 - val_mse: 7.3869 - lr: 1.0000e-04\n",
      "Epoch 48/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.5804 - mse: 0.6440\n",
      "New peak val_mae reached:   1.71\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466805.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.5807 - mse: 0.6446 - val_loss: 1.7091 - val_mse: 7.3442 - lr: 2.0000e-05\n",
      "Epoch 49/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.5506 - mse: 0.5453 - val_loss: 1.7165 - val_mse: 7.5964 - lr: 2.0000e-05\n",
      "Epoch 50/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.4765 - mse: 0.4233 - val_loss: 1.7155 - val_mse: 7.5135 - lr: 2.0000e-05\n",
      "Epoch 51/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4894 - mse: 0.4379\n",
      "New peak val_mae reached:   1.68\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466826.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.4882 - mse: 0.4363 - val_loss: 1.6781 - val_mse: 7.1662 - lr: 2.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.4850 - mse: 0.3972 - val_loss: 1.6862 - val_mse: 7.2527 - lr: 2.0000e-05\n",
      "Epoch 53/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.4239 - mse: 0.3118 - val_loss: 1.6856 - val_mse: 7.3562 - lr: 2.0000e-05\n",
      "Epoch 54/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.4386 - mse: 0.3390 - val_loss: 1.7560 - val_mse: 7.0428 - lr: 2.0000e-05\n",
      "Epoch 55/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.4734 - mse: 0.3774 - val_loss: 1.7332 - val_mse: 6.9676 - lr: 2.0000e-05\n",
      "Epoch 56/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.4700 - mse: 0.3582\n",
      "New peak val_mae reached:   1.66\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612466859.23456.h5\n",
      "waiting epochs reset.\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.4755 - mse: 0.3715 - val_loss: 1.6603 - val_mse: 6.9615 - lr: 2.0000e-05\n",
      "Epoch 57/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.4071 - mse: 0.2902 - val_loss: 1.7258 - val_mse: 6.9728 - lr: 2.0000e-05\n",
      "Epoch 58/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.4234 - mse: 0.2807 - val_loss: 1.6985 - val_mse: 6.9648 - lr: 2.0000e-05\n",
      "Epoch 59/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3961 - mse: 0.2754 - val_loss: 1.6875 - val_mse: 7.2719 - lr: 2.0000e-05\n",
      "Epoch 60/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.4379 - mse: 0.3106 - val_loss: 1.6743 - val_mse: 7.1308 - lr: 2.0000e-05\n",
      "Epoch 61/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.4254 - mse: 0.3064 - val_loss: 1.6838 - val_mse: 6.9910 - lr: 2.0000e-05\n",
      "Epoch 62/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.3691 - mse: 0.2424 - val_loss: 1.6590 - val_mse: 7.0306 - lr: 2.0000e-05\n",
      "Epoch 63/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3585 - mse: 0.2248 - val_loss: 1.6618 - val_mse: 6.9867 - lr: 2.0000e-05\n",
      "Epoch 64/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.4343 - mse: 0.3025 - val_loss: 1.6872 - val_mse: 7.0569 - lr: 2.0000e-05\n",
      "Epoch 65/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3790 - mse: 0.2382 - val_loss: 1.6704 - val_mse: 7.1659 - lr: 2.0000e-05\n",
      "Epoch 66/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3712 - mse: 0.2327\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3729 - mse: 0.2356 - val_loss: 1.7501 - val_mse: 7.0172 - lr: 2.0000e-05\n",
      "Epoch 67/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3323 - mse: 0.1967 - val_loss: 1.6647 - val_mse: 7.1432 - lr: 4.0000e-06\n",
      "Epoch 68/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3152 - mse: 0.1676 - val_loss: 1.6610 - val_mse: 7.1313 - lr: 4.0000e-06\n",
      "Epoch 69/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3028 - mse: 0.1652 - val_loss: 1.6558 - val_mse: 7.0612 - lr: 4.0000e-06\n",
      "Epoch 70/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3324 - mse: 0.1912 - val_loss: 1.6531 - val_mse: 7.0207 - lr: 4.0000e-06\n",
      "Epoch 71/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3492 - mse: 0.2040 - val_loss: 1.6575 - val_mse: 7.0599 - lr: 4.0000e-06\n",
      "Epoch 72/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2943 - mse: 0.1534 - val_loss: 1.6572 - val_mse: 7.0269 - lr: 4.0000e-06\n",
      "Epoch 73/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3202 - mse: 0.1736 - val_loss: 1.6544 - val_mse: 7.0790 - lr: 4.0000e-06\n",
      "Epoch 74/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3144 - mse: 0.1681 - val_loss: 1.6620 - val_mse: 7.0114 - lr: 4.0000e-06\n",
      "Epoch 75/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2974 - mse: 0.1444 - val_loss: 1.6932 - val_mse: 6.9152 - lr: 4.0000e-06\n",
      "Epoch 76/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3072 - mse: 0.1585\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.3089 - mse: 0.1600 - val_loss: 1.6604 - val_mse: 6.9926 - lr: 4.0000e-06\n",
      "Epoch 77/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.2921 - mse: 0.1451 - val_loss: 1.6586 - val_mse: 6.9812 - lr: 8.0000e-07\n",
      "Epoch 78/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2898 - mse: 0.1395 - val_loss: 1.6565 - val_mse: 6.9529 - lr: 8.0000e-07\n",
      "Epoch 79/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2905 - mse: 0.1542 - val_loss: 1.6568 - val_mse: 6.9831 - lr: 8.0000e-07\n",
      "Epoch 80/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2932 - mse: 0.1335 - val_loss: 1.6565 - val_mse: 6.9880 - lr: 8.0000e-07\n",
      "Epoch 81/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2904 - mse: 0.1505 - val_loss: 1.6571 - val_mse: 6.9655 - lr: 8.0000e-07\n",
      "Epoch 82/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2975 - mse: 0.1634 - val_loss: 1.6550 - val_mse: 7.0084 - lr: 8.0000e-07\n",
      "Epoch 83/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2777 - mse: 0.1374 - val_loss: 1.6593 - val_mse: 6.9606 - lr: 8.0000e-07\n",
      "Epoch 84/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2608 - mse: 0.1113 - val_loss: 1.6527 - val_mse: 7.0088 - lr: 8.0000e-07\n",
      "Epoch 85/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3022 - mse: 0.1540 - val_loss: 1.6557 - val_mse: 6.9750 - lr: 8.0000e-07\n",
      "Epoch 86/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2945 - mse: 0.1455\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2983 - mse: 0.1518 - val_loss: 1.6603 - val_mse: 6.9813 - lr: 8.0000e-07\n",
      "Epoch 87/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2552 - mse: 0.1176 - val_loss: 1.6564 - val_mse: 6.9891 - lr: 1.6000e-07\n",
      "Epoch 88/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2778 - mse: 0.1249 - val_loss: 1.6554 - val_mse: 7.0002 - lr: 1.6000e-07\n",
      "Epoch 89/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3061 - mse: 0.1638 - val_loss: 1.6539 - val_mse: 7.0052 - lr: 1.6000e-07\n",
      "Epoch 90/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2626 - mse: 0.1099 - val_loss: 1.6547 - val_mse: 6.9946 - lr: 1.6000e-07\n",
      "Epoch 91/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2520 - mse: 0.1048 - val_loss: 1.6549 - val_mse: 6.9798 - lr: 1.6000e-07\n",
      "Epoch 92/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2653 - mse: 0.1167 - val_loss: 1.6551 - val_mse: 6.9801 - lr: 1.6000e-07\n",
      "Epoch 93/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2431 - mse: 0.1081 - val_loss: 1.6562 - val_mse: 6.9672 - lr: 1.6000e-07\n",
      "Epoch 94/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2881 - mse: 0.1378 - val_loss: 1.6551 - val_mse: 6.9733 - lr: 1.6000e-07\n",
      "Epoch 95/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.2617 - mse: 0.1131 - val_loss: 1.6554 - val_mse: 6.9755 - lr: 1.6000e-07\n",
      "Epoch 96/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2997 - mse: 0.1510\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2995 - mse: 0.1508 - val_loss: 1.6548 - val_mse: 6.9794 - lr: 1.6000e-07\n",
      "Epoch 97/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2519 - mse: 0.1089 - val_loss: 1.6547 - val_mse: 6.9811 - lr: 3.2000e-08\n",
      "Epoch 98/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.2644 - mse: 0.1143 - val_loss: 1.6541 - val_mse: 6.9845 - lr: 3.2000e-08\n",
      "Epoch 99/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2614 - mse: 0.1129 - val_loss: 1.6543 - val_mse: 6.9825 - lr: 3.2000e-08\n",
      "Epoch 100/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2743 - mse: 0.1281 - val_loss: 1.6546 - val_mse: 6.9890 - lr: 3.2000e-08\n",
      "Epoch 101/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2882 - mse: 0.1397 - val_loss: 1.6544 - val_mse: 6.9796 - lr: 3.2000e-08\n",
      "Epoch 102/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2455 - mse: 0.0970 - val_loss: 1.6547 - val_mse: 6.9761 - lr: 3.2000e-08\n",
      "Epoch 103/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2927 - mse: 0.1554 - val_loss: 1.6555 - val_mse: 6.9714 - lr: 3.2000e-08\n",
      "Epoch 104/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.2881 - mse: 0.1431 - val_loss: 1.6553 - val_mse: 6.9797 - lr: 3.2000e-08\n",
      "Epoch 105/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2784 - mse: 0.1351 - val_loss: 1.6547 - val_mse: 6.9861 - lr: 3.2000e-08\n",
      "Epoch 106/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3015 - mse: 0.1479\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 6.399999818995639e-09.\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.3017 - mse: 0.1482 - val_loss: 1.6541 - val_mse: 6.9870 - lr: 3.2000e-08\n",
      "Epoch 107/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2731 - mse: 0.1244 - val_loss: 1.6548 - val_mse: 6.9769 - lr: 6.4000e-09\n",
      "Epoch 108/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2889 - mse: 0.1432 - val_loss: 1.6549 - val_mse: 6.9794 - lr: 6.4000e-09\n",
      "Epoch 109/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.2574 - mse: 0.1168 - val_loss: 1.6542 - val_mse: 6.9802 - lr: 6.4000e-09\n",
      "Epoch 110/160\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.2654 - mse: 0.1200 - val_loss: 1.6539 - val_mse: 6.9767 - lr: 6.4000e-09\n",
      "Epoch 111/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2742 - mse: 0.1289 - val_loss: 1.6543 - val_mse: 6.9866 - lr: 6.4000e-09\n",
      "Epoch 112/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2821 - mse: 0.1431 - val_loss: 1.6552 - val_mse: 6.9731 - lr: 6.4000e-09\n",
      "Epoch 113/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2637 - mse: 0.1161 - val_loss: 1.6548 - val_mse: 6.9755 - lr: 6.4000e-09\n",
      "Epoch 114/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2964 - mse: 0.1608 - val_loss: 1.6547 - val_mse: 6.9788 - lr: 6.4000e-09\n",
      "Epoch 115/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2924 - mse: 0.1496 - val_loss: 1.6543 - val_mse: 6.9891 - lr: 6.4000e-09\n",
      "Epoch 116/160\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.2814 - mse: 0.1354\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 1.279999928271991e-09.\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2807 - mse: 0.1348 - val_loss: 1.6547 - val_mse: 6.9855 - lr: 6.4000e-09\n",
      "Epoch 117/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2810 - mse: 0.1347 - val_loss: 1.6554 - val_mse: 6.9841 - lr: 1.2800e-09\n",
      "Epoch 118/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2900 - mse: 0.1370 - val_loss: 1.6545 - val_mse: 6.9858 - lr: 1.2800e-09\n",
      "Epoch 119/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2968 - mse: 0.1518 - val_loss: 1.6542 - val_mse: 6.9921 - lr: 1.2800e-09\n",
      "Epoch 120/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2999 - mse: 0.1503 - val_loss: 1.6540 - val_mse: 6.9917 - lr: 1.2800e-09\n",
      "Epoch 121/160\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.2778 - mse: 0.1339 - val_loss: 1.6549 - val_mse: 6.9810 - lr: 1.2800e-09\n",
      "Epoch 122/160\n",
      "46/77 [================>.............] - ETA: 2s - loss: 0.2699 - mse: 0.1245"
     ]
    }
   ],
   "source": [
    "fine_tune_epochs = ARG_MAX_FINE_TUNING_EPOCHS\n",
    "total_epochs =  (history.epoch[-1]+1) + fine_tune_epochs\n",
    "\n",
    "# by default we use custom LR schedule\n",
    "callbacks=[callbackObj, callbackLR]\n",
    "if ARG_LR_SCHEDULE == 1:\n",
    "    callbacks=[callbackObj, reduce_lr]\n",
    "    \n",
    "history_fine = model.fit(tmp_train_batches,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  history.epoch[-1]+1,\n",
    "                         validation_data=tmp_validation_batches,\n",
    "                         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjiNEG47yFVY"
   },
   "outputs": [],
   "source": [
    "print(history_fine.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpA8PlpQKygw"
   },
   "outputs": [],
   "source": [
    "mse += history_fine.history['mse']\n",
    "val_mse += history_fine.history['val_mse']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Igm8aMrLyFVj"
   },
   "outputs": [],
   "source": [
    "mse = mse[1:]\n",
    "val_mse = val_mse[1:]\n",
    "loss = loss[1:]\n",
    "val_loss = val_loss[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chW103JUItdk"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(mse, label='Training MSE')\n",
    "plt.plot(val_mse, label='Validation MSE')\n",
    "plt.ylim([0, 30])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Fine Tuning starts')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Training and Validation MSE')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 5])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Fine Tuning Starts')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TZTwG7nhm0C"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
    "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
    "\n",
    "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
    "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQy9mVkuoFWj"
   },
   "outputs": [],
   "source": [
    "if isGoogleColab():\n",
    "    !ls -l \"/content/drive/MyDrive/Healthcare/Radioterapia/data/ciolaplata/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4tr0VXGyFVr"
   },
   "outputs": [],
   "source": [
    "print(f'Best saved model file = {callbackObj.saved_model_file}')\n",
    "saved_model = callbackObj.saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esaw0xjhyFVv"
   },
   "outputs": [],
   "source": [
    "# Evaluate the saved model on the train set which differs from values reported during training\n",
    "loss, mse = saved_model.evaluate(tmp_train_batches, verbose=2)\n",
    "print(\"Saved model, train loss: {:5.4f}\".format(loss))\n",
    "print('Saved model, train mse: {:5.4f}\\n'.format(mse))\n",
    "\n",
    "# Evaluate the saved model on the validation set which differs from values reported during training\n",
    "loss, mse = saved_model.evaluate(tmp_validation_batches, verbose=2)\n",
    "print(\"Saved model, validation loss: {:5.4f}\".format(loss))\n",
    "print('Saved model, validation mse: {:5.4f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qACCSCEAyFVz"
   },
   "outputs": [],
   "source": [
    "#result_batch = model.predict(tmp_train_batches)\n",
    "#reloaded_result_batch = reloaded_model.predict(tmp_train_batches)\n",
    "#print(abs(reloaded_result_batch - result_batch).max())\n",
    "#np.testing.assert_allclose(result_batch, reloaded_result_batch, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcEpqDWFyFV4"
   },
   "outputs": [],
   "source": [
    "# projects out just the first two components.\n",
    "if ARG_TEST_PARTITION:\n",
    "    tmp_test_batches = test_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "    print(tmp_test_batches)\n",
    "\n",
    "    # Evaluate the reloaded model on the test set\n",
    "    loss, mse = saved_model.evaluate(tmp_test_batches, verbose=1)\n",
    "    print(\"\\n\\nSaved model, test loss: {:5.4f}\".format(loss))\n",
    "    print('Saved model, test mse: {:5.4f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1oRiBayRPVt"
   },
   "outputs": [],
   "source": [
    "if isGoogleColab():\n",
    "    drive.flush_and_unmount()\n",
    "    print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "transfer_learning.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
